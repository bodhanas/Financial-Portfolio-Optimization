{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Assignment - Portfolio Generation\n",
    "\n",
    "### Team Number: 7\n",
    "### Team Members: Ashton, Bodhana, Johnson\n",
    "### Team Strategy: Risky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "from IPython.display import display, Math, Latex\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdate\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants\n",
    "\n",
    "We start by setting the following global constants:\n",
    "* CSV file of tickers\n",
    "* Start and end date interval we will be analyzing to assemble our final portfolio\n",
    "* End date is also used as the date we generate our final portfolio\n",
    "* Flat fee applied to each stock (NOT share) purchased\n",
    "* Number of stocks in our portfolio\n",
    "* Investment amount\n",
    "\n",
    "Our final portfolio will consist of 10 stocks (the minimum allowed) since the lower the number of stocks, the less diverse and ineffective at diversifying away non-systematic risk the portfolio will be. In simpler terms, losses in one stock are unlikely to be counteracted by gains in another when there are fewer stocks and vice versa, making these conditions more risky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticker CSV file\n",
    "ticker_file_name = 'Tickers_Example.csv'\n",
    "\n",
    "# Start and end dates for data analysis\n",
    "# Start on first trading day in 2022 and end on day of simulation\n",
    "# Period is reflective of current economic situation\n",
    "start_date = '2022-01-04'\n",
    "end_date = '2023-11-25'\n",
    "\n",
    "# Flat fee of $4.95 CAD charged on every stock in portfolio\n",
    "flat_fee = 4.95\n",
    "\n",
    "# Number of stocks chosen (less stocks -> less diversification)\n",
    "num_stocks = 10\n",
    "\n",
    "# Investment amount in CAD\n",
    "investment = 750_000 - (flat_fee * num_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Process\n",
    "\n",
    "First, it is imperative to define what is considered the \"riskiest\" portfolio. By \"riskiest,\" our group aimed to generate either the extreme highest or lowest percent returns on our initial investment. However, by definition, risk only describes the possibility of loss. Our data may reveal extremely high returns for a stock at one point in time, but with different market conditions, may also result in the great loss. Hence, we redefined risk for this assignment as **volatility** or the degree of a stock's fluctuation in the market, which constitutes both gains and losses. This is what we intend to maximize and measute; the greater the volatility, the greater the risk.\n",
    "\n",
    "To do so, we first minimized diversification by selecting the minimium number of stocks: 10. Then to select our 10 stocks, we relied on a combination of two metrics: **correlation** and **beta**, whose purposes we will discuss in depth further on.\n",
    "\n",
    "Our ideal objective is to the create a portfolio consisting of 10 stocks that exhibit perfect correlation to one another as we want all the stocks to move in the same direction. If all the stocks perform well, it would result in massive returns for the investor; however, if they do poorly, the investor could lose a significant amount of money. In other words, we want to avoid any stocks moving in opposite directions since that would support a diversified, risk-adverse portfolio.\n",
    "\n",
    "Apart from being correlated to one another, we want the selected stocks to be the most volatile relative to the market, but we needed to determine whether market correlation or beta was the best metric to measure this. Overall, in order to figure out whether correlation between stocks, market correlation, beta, or a combined measure would yield the best performance, we designed and tested five strategies on past stock data, before selecting a final strategy based on which one generated the most volatile portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Tickers\n",
    "\n",
    "We generate a list of tickers from the CSV file. The raw tickers are then filtered based on the given requirements:\n",
    "\n",
    "* Ignore tickers that do not reference a valid stock denominated in either USD or CAD\n",
    "* Only include stocks in your portfolio that have at least 150,000 shares of average monthly volume, as calculated based on the time interval of January 1, 2023 to October 31, 2023.\n",
    "* Drop any month that does not have at least 18 trading days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of tickers from the CSV file\n",
    "raw_tickers = pd.read_csv(ticker_file_name, header = None)[0].tolist()\n",
    "print(raw_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the file and return a list of valid stocks\n",
    "\n",
    "# Start and end dates used for the filter calculations\n",
    "required_start_for_filter = \"2023-01-01\"\n",
    "required_end_for_filter = \"2023-10-01\"\n",
    "\n",
    "# Minimum trading days and volume\n",
    "minimum_days = 18\n",
    "minimum_avg_volume = 150000\n",
    "\n",
    "# List of valid exchange tickers\n",
    "valid_us_exchanges = ['NMS', 'BTS', 'IEX', 'NAS', 'ASE', 'PCX', 'NYQ', 'OPR', 'OBB', 'PNK']\n",
    "valid_can_exchanges = ['CSE', 'NEO', 'TOR', 'VAN']\n",
    "\n",
    "# Empty list to store the valid tickers\n",
    "valid_tickers = []\n",
    "\n",
    "# Consumes a stock ticker and checks whether it is a valid stock based on assignment rules\n",
    "def validate_stock(stock):\n",
    "\n",
    "    stock_info = yf.Ticker(stock).fast_info\n",
    "\n",
    "    # Accomodate for any tickers that give an error\n",
    "    try:\n",
    "\n",
    "        # Check if the stock is denominated in USD or CAD and is trading on a valid exchange\n",
    "        if ((stock_info['currency'] == 'USD' or stock_info['currency'] == 'CAD') and\n",
    "            (stock_info['exchange'] in valid_us_exchanges or stock_info['exchange'] in valid_can_exchanges)):\n",
    "\n",
    "            # Get stock's historical data\n",
    "            stock_hist = yf.Ticker(stock).history(start=required_start_for_filter, end=required_end_for_filter).dropna()\n",
    "\n",
    "            # Create a dataframe of the number of trading days per month\n",
    "            monthly_trading_days = stock_hist['Volume'].groupby(pd.Grouper(freq = 'MS')).count()\n",
    "\n",
    "            # Create a dataframe of the stock's total monthly volume\n",
    "            monthly_volume = stock_hist['Volume'].groupby(pd.Grouper(freq = 'MS')).sum()\n",
    "\n",
    "            # Drop the months in the volumes which have less than the minimum amount of trading days\n",
    "            for month in monthly_trading_days.index:\n",
    "\n",
    "                num_of_days = monthly_trading_days.loc[month]\n",
    "\n",
    "                if num_of_days < minimum_days:\n",
    "                    monthly_volume.drop(month, inplace = True)\n",
    "\n",
    "            # If the monthly volume requirement is met, add it to the list\n",
    "            if monthly_volume.mean() >= minimum_avg_volume:\n",
    "                valid_tickers.append(stock)\n",
    "\n",
    "            else:\n",
    "                print(f\"{stock} did not meet the volume requirements.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"{stock} is not in a Canadian or U.S. stock market.\")\n",
    "\n",
    "    except:\n",
    "        print(f\"{stock} is not valid.\")\n",
    "\n",
    "for ticker in raw_tickers:\n",
    "    validate_stock(ticker)\n",
    "\n",
    "print(f'Valid Tickers: {valid_tickers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Close Prices of Stocks\n",
    "\n",
    "The close prices for the valid tickers are added to a common dataframe. Since our investment is in CAD, the stocks denominated in USD will be converted to CAD using the exchange rate. This data will be used to calculate the historical percent returns of the stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add close prices to a dataframe\n",
    "\n",
    "# Consumes a list of stock tickers and an empty dataframe and produces a dataframe of the stocks' close prices\n",
    "def add_close_prices(valid_tickers, close_price_data):\n",
    "\n",
    "    for ticker in valid_tickers:\n",
    "        close_price_data[ticker] = yf.Ticker(ticker).history(start = start_date, end = end_date, interval = '1d')['Close']\n",
    "\n",
    "    return close_price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert close prices of stocks denominated in USD to CAD\n",
    "\n",
    "# Get USD -> CAD exchange data\n",
    "exchange_close_data = pd.DataFrame(yf.Ticker(\"USDCAD=x\").history(start = start_date, end = end_date, interval = '1d')['Close'])\n",
    "exchange_close_data.rename(columns = {'Close':'1 USD = ? CAD'}, inplace = True)\n",
    "\n",
    "# Remove timestamp from indices and retain date\n",
    "# Enables comparison between the close price and exchange dataframes since the indices are in the same format\n",
    "exchange_close_data.index = exchange_close_data.index.strftime('%Y-%m-%d')\n",
    "\n",
    "# Consumes the close prices and exchange dataframes\n",
    "# Returns the close prices dataframe with USD converted to CAD where applicable\n",
    "def convert_USDtoCAD(close_price_data, exchange_close_data):\n",
    "\n",
    "    for col in close_price_data.columns:\n",
    "\n",
    "        stock_info = yf.Ticker(col).fast_info\n",
    "\n",
    "        # Check if the stock is denominated in USD and convert to CAD if so\n",
    "        if (stock_info['currency'] == 'USD'):\n",
    "            close_price_data[col] = close_price_data[col] * exchange_close_data['1 USD = ? CAD']\n",
    "\n",
    "    return close_price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the adjusted close prices (USD -> CAD)\n",
    "\n",
    "# Consumes a list of stock tickers and exchange dataframe and produces a dataframe of the stocks' adjusted close prices\n",
    "def create_close_price_dataframe(valid_tickers, exchange_close_data):\n",
    "\n",
    "    close_price_data = pd.DataFrame()\n",
    "\n",
    "    # Add raw close prices\n",
    "    close_price_data = add_close_prices(valid_tickers, close_price_data)\n",
    "\n",
    "    # Normalize index to index of exchange dataframe (same type)\n",
    "    close_price_data.index = close_price_data.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Extract exchange data for ONLY trading days\n",
    "    exchange_close_data = exchange_close_data[exchange_close_data.index.isin(close_price_data.index)]\n",
    "\n",
    "    # Convert stocks denominated in USD to CAD\n",
    "    convert_USDtoCAD(close_price_data, exchange_close_data)\n",
    "\n",
    "    return close_price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate STOCK CLOSE PRICES dataframe and display\n",
    "close_price_data = create_close_price_dataframe(valid_tickers, exchange_close_data)\n",
    "close_price_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Percent Returns of Stocks\n",
    "\n",
    "Using the close prices, the percent returns of each stock are added to a common dataframe to later calculate correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of percent returns\n",
    "\n",
    "# Consumes the close price dataframe and produces a dataframe of the stocks' adjusted close prices\n",
    "def create_stock_return_dataframe(close_price_data):\n",
    "\n",
    "    stock_return_data = pd.DataFrame()\n",
    "\n",
    "    for stock in close_price_data.columns:\n",
    "\n",
    "      # Calculate percent return of stock and add to dataframe\n",
    "      stock_return_data[stock] = close_price_data[stock].pct_change()\n",
    "\n",
    "    # Drop day one (NaN value since previous value not in date range to compare)\n",
    "    stock_return_data.drop(index = stock_return_data.index[0], inplace = True)\n",
    "\n",
    "    return stock_return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate STOCK RETURN dataframe and display\n",
    "stock_return_data = create_stock_return_dataframe(close_price_data)\n",
    "stock_return_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Weightings\n",
    "\n",
    "We started from first principles and determined that in order to maximize volatility, we should maximize the amount invested into the most volatile stock and progress down the list. After consulting with past CFM 101 students, we arrived at the following code to calculate our weights and comply with the requirements to cap a stock at 25% of the portfolio and have a minimum of 100/2n where n is the number of stocks (n = 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate optimal weightings for 10 stocks in order of volatility\n",
    "\n",
    "max_weight = 20\n",
    "min_weight = 100 / (2 * num_stocks)\n",
    "\n",
    "# Weight already used for prior stocks in list\n",
    "total_weight = 0\n",
    "\n",
    "# Empty list to store the optimal weights\n",
    "optimal_weights_lst = []\n",
    "\n",
    "# Number of stocks left after one subtracted every time loop runs\n",
    "stocks_left = num_stocks\n",
    "\n",
    "while (stocks_left != 0):\n",
    "\n",
    "    # Weight of next stock on list\n",
    "    next_weight = min_weight * (stocks_left - 1)\n",
    "    # Weight left after next weighting is applied\n",
    "    rem_weight = 100 - next_weight - total_weight\n",
    "\n",
    "    if (rem_weight > max_weight):\n",
    "        optimal_weights_lst.append(max_weight / 100)\n",
    "        total_weight += max_weight\n",
    "        \n",
    "    elif (rem_weight > min_weight):\n",
    "        optimal_weights_lst.append(rem_weight / 100)\n",
    "        total_weight += rem_weight\n",
    "        \n",
    "    else: # rem_weight <= min_weight\n",
    "        optimal_weights_lst.append(min_weight / 100)\n",
    "        total_weight += min_weight\n",
    "\n",
    "    stocks_left -= 1\n",
    "\n",
    "print(optimal_weights_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 0\n",
    "\n",
    "To select the 10 most volatile stocks, a brute force solution is not possible. The number of possibilities would be x choose 10 with x being the number of candidate stocks. For example, even a relatively small x-value of 50 would lead to the number of possibilities spanning over 12 billion stock combinations. This is O(n^(x-10)) time complexity, reasonable with a dedicated computer for running the calculations, but impractical for the purposes of this assignment. Even looping from 0 to 12 billion was a lengthy computation for Python.\n",
    "\n",
    "Hence, we shifted our focus to finding a heuristic solution or a strategy not guaranteed to find the optimal solution but a satisfactory one in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1\n",
    "\n",
    "This strategy is built on the assumption that given Stock A, all other stocks that demonstrate a high correlation to Stock A have a high correlation amongst themselves.\n",
    "\n",
    "We consider this to be a fair assumption. In an ideal world, we would invest in a singular stock, but that is not possible based on the requierements for this assignment. Thus, we want all 10 of our stocks to be very similar to carbon copies of each other. Once we select one of the stocks, then create a portfolio with the other stock, since they are carbon copies, the portfolio will not change. To reiterate, comparing the third carbon copy stock and so on would not result in a change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of correlations between 2 stocks or correlations matrix\n",
    "\n",
    "# Consumes the stock return dataframe and produces a correlations matrix\n",
    "def create_stock_correl_dataframe(stock_return_data):\n",
    "\n",
    "    correl_data = pd.DataFrame()\n",
    "\n",
    "    # Calculate correlation using the pearson method\n",
    "    # Source: https://www.geeksforgeeks.org/python-pandas-dataframe-corr/\n",
    "    correl_data = stock_return_data.corr(method = \"pearson\")\n",
    "\n",
    "    return correl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate CORRELATIONS MATRIX dataframe and display\n",
    "stock_correl_data = create_stock_correl_dataframe(stock_return_data)\n",
    "stock_correl_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average of the negative correlation values\n",
    "# Large magnitude (close to -1) would indicate that the bottom 10 values should also be considered\n",
    "\n",
    "neg_num_lst = []\n",
    "\n",
    "for col in stock_correl_data:\n",
    "    for val in stock_correl_data[col]:\n",
    "        # Get negative correlation values and add them to empty list\n",
    "        if (val < 0):\n",
    "            neg_num_lst.append(val)\n",
    "\n",
    "np.mean(neg_num_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do recognize that stocks may be negatively correlated to one another and hence the bottom 10 stocks may consist a riskier portfolio than the top 10. However, the probability of this occurring is slim (as seen by the extremely low average of the negative correlation values for the randomized stocks given in the CSV file). As a result, an assumption has been made that we can also expect randomized stocks during the simulation and hence the redundant implementation has been omitted.\n",
    "\n",
    "This assumption carries over to other correlations calculated in subsequent strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a list of 9 stocks that are best correlated to a singular stock\n",
    "\n",
    "max_sum = 0\n",
    "top10_dict = {}\n",
    "\n",
    "for stock in stock_correl_data.columns:\n",
    "\n",
    "    # Get the sum of the top 10 correlation values to each stock\n",
    "    # Subtract one to get rid of the correlation value = 1 with itself\n",
    "    current_col_sum = stock_correl_data[stock].nlargest(10).sum() - 1\n",
    "\n",
    "    # If current sum is larger than the max so far\n",
    "    if current_col_sum > max_sum:\n",
    "        max_sum = current_col_sum\n",
    "        # Add the top 10 correlation values for the stock to a list\n",
    "        top10_dict = stock_correl_data[stock].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_df = pd.DataFrame(top10_dict.items(), columns = [\"Ticker\", \"Correlation\"])\n",
    "top10_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final tickers from Strategy 1\n",
    "s1_tickers = list(top10_df[\"Ticker\"])\n",
    "print(s1_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2\n",
    "\n",
    "This strategy uses dynamic programming. The problem at hand is finding the stock that correlates the most to the current portfolio, adding it to the portfolio, and then finding the next stock that correlates with the portfolio that was just created.\n",
    "\n",
    "This does not work on all data sets, because it is possible for the algorithm to get trapped in a local minimum. For example, if there were 5 stocks, where 2 of them are very closely correlated and the 3 other stocks are not, the model would dodge choosing the 3 stocks even if they are better correlated than the 2 very correlated stocks and a third non-correlated stock.\n",
    "\n",
    "Despite this limitation, we thought that this could be a reasonable approach because in the real world, the correlations between stocks would be more random and it would be unlikely to have a single pair of closely correlated stocks with the rest moving in the opposite direction.\n",
    "\n",
    "As depicted in the graph near the end of this file, this algorithm did not generate the most volatile portfolio for 10 stocks. Yet, it would be fascinating to use this strategy when a portfolio is scaled up (more stocks in the mix). The time complexity is O(n^2) so it will be a computationally viable option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pair with the highest correlation\n",
    "# Combine pair into one portfolio\n",
    "# Find the next stock, by taking the one with the most correlation to the portfolio\n",
    "\n",
    "max_corr = 0\n",
    "\n",
    "pair = []\n",
    "\n",
    "stockPortfolio = pd.DataFrame()\n",
    "\n",
    "for stock in stock_correl_data.columns:\n",
    "    \n",
    "    current_col_sum = stock_correl_data[stock].nlargest(2).sum() - 1\n",
    "    \n",
    "    if current_col_sum > max_corr:\n",
    "        max_corr = current_col_sum\n",
    "        pair = [stock, stock_correl_data[stock].nlargest(2).index[1]]\n",
    "\n",
    "# Test code\n",
    "# print(max_corr)\n",
    "# print(pair)\n",
    "\n",
    "stocks = stock_return_data\n",
    "\n",
    "startingPortfolio = stocks[pair[0]] * 0.5 + stocks[pair[1]] * 0.5\n",
    "\n",
    "curAssetDict = {}\n",
    "curAssetDict[pair[0]] = True\n",
    "curAssetDict[pair[1]] = True\n",
    "\n",
    "def findMax(stocks, curPortfolio, numAssets, curAssetDict):\n",
    "\n",
    "    if numAssets == 10:\n",
    "        return curPortfolio\n",
    "\n",
    "    runningMax = 0\n",
    "\n",
    "    maxCandidate = pd.DataFrame()\n",
    "\n",
    "    for stock in stock_return_data.columns:\n",
    "\n",
    "        if stock in curAssetDict:\n",
    "            continue\n",
    "\n",
    "        correlation = stock_return_data[stock].corr(curPortfolio)\n",
    "\n",
    "        if correlation > runningMax:\n",
    "            runningMax = correlation\n",
    "            maxCandidate = stock_return_data[stock]\n",
    "\n",
    "    # Create equally-weighted portfolio\n",
    "    curPortfolio = curPortfolio * (numAssets / (numAssets + 1)) + (1 / (numAssets+1)) * maxCandidate\n",
    "\n",
    "    curAssetDict[maxCandidate.name] = True\n",
    "\n",
    "    findMax(stocks,curPortfolio, numAssets + 1, curAssetDict)\n",
    "\n",
    "findMax(stocks, startingPortfolio, 2, curAssetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final tickers from Strategy 2\n",
    "s2_tickers = list(curAssetDict)\n",
    "print(s2_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3\n",
    "\n",
    "We can also determine which stocks move in the same direction by comparing them to the movement of an market index. In this model, the 10 stocks that are most correlated to the market in terms of percent returns will constitute the portfolio.\n",
    "\n",
    "Due to the larger representation of stocks denominated in USD compared to CAD in the given CSV file, the assumption that the simulation file will follow a similar format was made. Accordingly, we decided to use the S&P500 index due to its reputation of being one of the \"best gauges of large U.S. stocks\" due to its depth and diversity (according to https://www.investopedia.com/terms/s/sp500.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the chosen market's returns\n",
    "\n",
    "# Consumes a market index ticker and produces a dataframe of market percent returns\n",
    "def create_market_return_dataframe(m_index):\n",
    "\n",
    "    market_return_data = m_index.history(start = start_date, end = end_date, interval = '1d')['Close'].pct_change()\n",
    "\n",
    "    market_return_data.index = market_return_data.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    market_return_data.drop(index = market_return_data.index[0], inplace = True)\n",
    "\n",
    "    return market_return_data\n",
    "\n",
    "# Create a dataframe of correlations between each stock and the chosen market\n",
    "\n",
    "# Consumes the market return and stock return dataframes\n",
    "# Produces a dataframe of their correlation values\n",
    "def create_market_correl_dataframe(market_return_data, stock_return_data):\n",
    "\n",
    "    market_correl_data = pd.DataFrame({\"Ticker\": stock_return_data.columns})\n",
    "    corr_list = []\n",
    "\n",
    "    for col in stock_return_data.columns:\n",
    "\n",
    "        corr_list.append(stock_return_data[col].corr(market_return_data))\n",
    "\n",
    "    market_correl_data[\"Correlation\"]  = corr_list\n",
    "    # Sort values from highest to lowest\n",
    "    market_correl_data.sort_values(by = ['Correlation'], ascending = False, ignore_index = True, inplace = True)\n",
    "\n",
    "    return market_correl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select S&P500 as market index\n",
    "sp500 = yf.Ticker('^GSPC')\n",
    "\n",
    "market_return_data = create_market_return_dataframe(sp500)\n",
    "market_correl_data = create_market_correl_dataframe(market_return_data, stock_return_data)\n",
    "\n",
    "top10_df2 = market_correl_data.head(10)\n",
    "top10_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final tickers from Strategy 3\n",
    "s3_tickers = list(top10_df2[\"Ticker\"])\n",
    "print(s3_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 4\n",
    "\n",
    "This approach utilizes beta to determine the volatility of stocks in a given market. While correlation measures the tendency of 2 stocks to move in the same direction, it does not account for the relative size of these directional moves; is one stock losing or gaining value more than the other? Beta, on the other hand, accounts for both direction and relative volatility and can therefore be more insightful.\n",
    "\n",
    "As with Strategy 3, we will be determining the 10 stocks with the largest beta values, which indicate that they are more volatile relative to the market. To calculate beta, we will be using the following formula, where the co-variance of the stock's return and market's return is divided by the variance of the market returns.\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta=\\frac{COV(x_i,r_M)}{\\sigma^2(r_M)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of the beta of each stock in relation to the chosen market\n",
    "\n",
    "# Consumes the market return and stock return dataframes\n",
    "# Produces a dataframe of the stocks' beta values\n",
    "def create_beta_dataframe(market_return_data, stock_return_data):\n",
    "    \n",
    "    # Calculate the variance of the market returns\n",
    "    market_var = market_return_data.var()\n",
    "    \n",
    "    beta_data = pd.DataFrame({\"Ticker\": stock_return_data.columns})\n",
    "    \n",
    "    beta_list = []\n",
    "\n",
    "    for col in stock_return_data.columns:\n",
    "        \n",
    "        # Calculate beta using formula\n",
    "        beta_list.append((stock_return_data[col].cov(market_return_data)) / market_var)\n",
    "\n",
    "    beta_data[\"Beta\"]  = beta_list\n",
    "    \n",
    "    beta_data.sort_values(by = ['Beta'], ascending = False, ignore_index = True, inplace = True)\n",
    "\n",
    "    return beta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_data = create_beta_dataframe(market_return_data, stock_return_data)\n",
    "\n",
    "top10_df3 = beta_data.head(10)\n",
    "top10_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHOP.TO has the highest beta, but can be considered an outlier due to the unique circumstances surrounding the business. In addition to being a Canadian stock which may not be represented well by the S&P500, the company's valuation has also been on the decline leading to significant fluctuations in stock prices due to constant lay-offs.\n",
    "\n",
    "Regardless, this strategy is the top contender as it is capable of detecting SHOP.TO, which is well-known as a high-risk stock (which the previous strategies overlooked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final tickers from Strategy 4\n",
    "s4_tickers = list(top10_df3[\"Ticker\"])\n",
    "s4_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 5\n",
    "\n",
    "This strategy takes Strategy 3, where we found the 10 most correlated stocks to the S&P500, and combines it with Strategy 4, where we found the 10 stocks with the highest beta. By ensuring that the stocks are correlated with the S&P500 and are weighted by their volatility via beta, we aim to build a portfolio that not only moves in the same direction but the stocks with larger magnitudes of change get larger weights in the investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the return data of the 10 most correlated stocks from Strategy 3\n",
    "# Calculate their betas and order them from greatest to least\n",
    "\n",
    "correlated_stocks_w_beta = create_beta_dataframe(market_return_data, create_stock_return_dataframe(create_close_price_dataframe(s3_tickers, exchange_close_data)))\n",
    "correlated_stocks_w_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final tickers from Strategy 5\n",
    "s5_tickers = correlated_stocks_w_beta.Ticker.tolist()\n",
    "print(s5_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Generation\n",
    "\n",
    "Next up is generating portfolios for all our strategies which should contain:\n",
    "\n",
    "* Ticker\n",
    "* Price\n",
    "* Currency\n",
    "* Shares\n",
    "* Value\n",
    "* Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumes the list of stocks generated by a strategy, investment, and date, and produces a portfolio dataframe\n",
    "def generate_portfolio(tickers, investment, date):\n",
    "\n",
    "    portfolio = pd.DataFrame()\n",
    "\n",
    "    shares_in_each = []\n",
    "    price_of_each = []\n",
    "    investment_lst = []\n",
    "    currency_lst = []\n",
    "\n",
    "    for i in range(len(tickers)):\n",
    "\n",
    "        investment_lst.append((optimal_weights_lst[i] * investment))\n",
    "        shares_in_each.append(investment_lst[i] / close_price_data.loc[date, tickers[i]])\n",
    "        price_of_each.append(close_price_data.loc[date , tickers[i]])\n",
    "        \n",
    "        stock_info = yf.Ticker(tickers[i]).fast_info\n",
    "        if (stock_info['currency'] == 'USD'):\n",
    "            currency_lst.append('USD')\n",
    "        else:\n",
    "            currency_lst.append('CAD')\n",
    "\n",
    "    portfolio['Ticker'] = tickers\n",
    "    portfolio['Price'] = price_of_each\n",
    "    portfolio['Currency'] = currency_lst\n",
    "    portfolio['Shares'] = shares_in_each\n",
    "    portfolio['Value'] = investment_lst\n",
    "    portfolio['Weight'] = optimal_weights_lst\n",
    "\n",
    "    portfolio.index += 1\n",
    "\n",
    "    return portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio for Strategy 1\n",
    "portfolio1 = generate_portfolio(s1_tickers, investment, start_date)\n",
    "portfolio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio for Strategy 2\n",
    "portfolio2 = generate_portfolio(s2_tickers, investment, start_date)\n",
    "portfolio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio for Strategy 3\n",
    "portfolio3 = generate_portfolio(s3_tickers, investment, start_date)\n",
    "portfolio3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio for Strategy 4\n",
    "portfolio4 = generate_portfolio(s4_tickers, investment, start_date)\n",
    "portfolio4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio for Strategy 5\n",
    "portfolio5 = generate_portfolio(s5_tickers, investment, start_date)\n",
    "portfolio5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Strategies\n",
    "\n",
    "Using the portfolios from above, we next generated dataframes to track the growth of the investment value over time (date range given by the start and end dates specified as global constants) if we had invested at a past date. By analyzing this data using visual (graph) and statistical (standard deviation) metrics, we can discover the portfolio with the greatest volatility and ultimately risk; this will be the final portfolio we choose for the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total shares and value of each portfolio over an interval\n",
    "\n",
    "# Consumes the list of stocks generated by a strategy, close price data dataframe, and the strategy's portfolio\n",
    "# Produces dataframe of historical test data if the investment had been made at an earlier date with the given strategy\n",
    "def create_test_portfolio(tickers, close_price_data, portfolio_df):\n",
    "\n",
    "    test_portfolio_df = pd.DataFrame()\n",
    "    i = 1 # Starting index of stock in portfolio dataframe\n",
    "\n",
    "    for t in tickers:\n",
    "        for col in close_price_data:\n",
    "            if t == col:\n",
    "                test_portfolio_df[col] = close_price_data[col] * portfolio_df[\"Shares\"][i]\n",
    "                i += 1\n",
    "\n",
    "    test_portfolio_df[\"Total Value\"] =  test_portfolio_df.sum(axis = 1)\n",
    "\n",
    "    return test_portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Portfolio for Strategy 1\n",
    "test_portfolio1 = create_test_portfolio(s1_tickers, close_price_data, portfolio1)\n",
    "test_portfolio1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Portfolio for Strategy 2\n",
    "test_portfolio2 = create_test_portfolio(s2_tickers, close_price_data, portfolio2)\n",
    "test_portfolio2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Portfolio for Strategy 3\n",
    "test_portfolio3 = create_test_portfolio(s3_tickers, close_price_data, portfolio3)\n",
    "test_portfolio3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Portfolio for Strategy 4\n",
    "test_portfolio4 = create_test_portfolio(s4_tickers, close_price_data, portfolio4)\n",
    "test_portfolio4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Portfolio for Strategy 5\n",
    "test_portfolio5 = create_test_portfolio(s5_tickers, close_price_data, portfolio5)\n",
    "test_portfolio5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the test portfolios\n",
    "\n",
    "plt.figure(figsize = (30, 5))\n",
    "\n",
    "# Strategy 1\n",
    "plt.plot(test_portfolio1.index, test_portfolio1['Total Value'], label='S1: Correlation Between 2 Stocks Portfolio')\n",
    "\n",
    "# Strategy 2\n",
    "plt.plot(test_portfolio2.index, test_portfolio2['Total Value'], label='S2: DP Portfolio')\n",
    "\n",
    "# Strategy 3\n",
    "plt.plot(test_portfolio3.index, test_portfolio3['Total Value'], label='S3: Correlation Between Stock and S&P500 Portfolio')\n",
    "\n",
    "# Strategy 4\n",
    "plt.plot(test_portfolio4.index, test_portfolio4['Total Value'], label = 'S4: Beta Portfolio')\n",
    "\n",
    "# Strategy 5\n",
    "plt.plot(test_portfolio5.index, test_portfolio5['Total Value'], label = 'S5: Correlation Between Stock and S&P500, Weighted by Beta')\n",
    "\n",
    "locator = mdate.MonthLocator()\n",
    "plt.gca().xaxis.set_major_locator(locator)\n",
    "\n",
    "plt.title('Comparison of Portfolios')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of each portfolio's percent returns\n",
    "\n",
    "test_portfolios = [test_portfolio1, test_portfolio2, test_portfolio3, test_portfolio4, test_portfolio5]\n",
    "\n",
    "# Consumes a test portfolio\n",
    "# Returns the standard deviations of the percent returns of the total values of investment over time\n",
    "def get_test_percent_returns(test_portfolio):\n",
    "\n",
    "    # Create a dataframe with the portfolio's total value\n",
    "    pct_returns = pd.DataFrame()\n",
    "\n",
    "    pct_returns['Value'] = portfolio['Total Value']\n",
    "\n",
    "    # Add a column for percent returns of the total value\n",
    "    pct_returns['% Returns'] = portfolio['Total Value'].pct_change()\n",
    "\n",
    "    pct_returns.drop(index = pct_returns.index[0], inplace = True)\n",
    "\n",
    "    # Calculate standard deviation of percent returns\n",
    "    std_dev_of_returns = pct_returns['% Returns'].std()\n",
    "\n",
    "    return std_dev_of_returns\n",
    "\n",
    "pNum = 0\n",
    "\n",
    "for portfolio in test_portfolios:\n",
    "    pNum += 1\n",
    "    print(f'Test Portfolio {pNum} Standard Deviation of % Returns: {(get_test_percent_returns(portfolio))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Strategy: Strategy 4: Beta\n",
    "\n",
    "It is clearly evident from the graph that Strategy 4 had the highest volatility, dropping in value the most overall from the initial investment and displaying several abrupt dips, despite trending in the same direction as the other stocks. This drop in value can be quantified using the absolute value of standard deviation(can also use variance) with a higher standard deviation indicating greater volatility and hence risk. Strategy 4 also resulted in the highest standard deviation of 0.0386, further cementing it as the \"riskiest\" strategy of the five tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Portfolio Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final portfolio, using strategy 4\n",
    "# Need to regenerate with \"2023-11-25\"\n",
    "Portfolio_Final = generate_portfolio(s4_tickers, investment, end_date)\n",
    "\n",
    "# Verifying the total and sum of weights\n",
    "final_portfolio_total = round(Portfolio_Final.Value.sum() + (flat_fee * num_stocks))\n",
    "final_portfolio_weights = round(portfolio4.Weight.sum())\n",
    "\n",
    "print(f'The total value of the portfolio is ${final_portfolio_total}')\n",
    "print(f'The sum of the weights is: {final_portfolio_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio_Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"Stock_Final\" dataframe\n",
    "Stocks_Final = pd.DataFrame()\n",
    "Stocks_Final['Ticker'] = Portfolio_Final['Ticker']\n",
    "Stocks_Final['Shares'] = Portfolio_Final['Shares']\n",
    "\n",
    "# Output the final stocks and the number of shares to a CSV\n",
    "Stocks_Final.to_csv(\"Stocks_Group_7.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution Declaration\n",
    "\n",
    "The following team members made a meaningful contribution to this assignment:\n",
    "\n",
    "Ashton\n",
    "\n",
    "Bodhana\n",
    "\n",
    "Johnson"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
